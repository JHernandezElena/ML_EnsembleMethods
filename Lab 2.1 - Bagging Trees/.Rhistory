set.seed(150) #For replication
#Train model using boostrapped trained data
BAGtree.fit = train(fTR[,c("X1","X2")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "treebag",        #  <------------------------------        BAGGING TREE
preProcess = c("center","scale"),
trControl = ctrl,
metric = "ROC")
library(caret)
library(ggplot2)
library(ROCR) #for plotting ROC curves.
library(MLTools)
## Load file -------------------------------------------------------------------------------------
fdata = read.table("SimData.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
str(fdata)
fdata$Y = as.factor(fdata$Y)
str(fdata)
## Divide the data into training and validation sets ---------------------------------------------------
set.seed(150) #For replication
ratioTR = 0.8 #Percentage for training
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
#obtain training and validation sets
fTR = fdata[trainIndex,]
fTV = fdata[-trainIndex,]
## Initialize trainControl -----------------------------------------------------------------------
#  no cross-validation method is used
ctrl <- trainControl(method = "none",                      # method= "none" when no resampling is used
summaryFunction = twoClassSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = twoClassSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
set.seed(150) #For replication
#Train model using boostrapped trained data
BAGtree.fit = train(fTR[,c("X1","X2")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "treebag",        #  <------------------------------        BAGGING TREE
preProcess = c("center","scale"),
trControl = ctrl,
metric = "ROC")
#Plot one individual tree (the third one)
plot(BAGtree.fit$finalModel$mtrees[[3]]$btree, uniform=TRUE,margin=0.2)
text(BAGtree.fit$finalModel$mtrees[[3]]$btree, use.n=TRUE, all=TRUE, cex=.5)
tree.fit = BAGtree.fit;
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTR) # predict classes
#validation
fTV_eval = fTV
fTV_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTV) # predict probabilities
fTV_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTV) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
tree.fit,#Fitted model with caret
var1="X1", var2="X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
# Validation
confusionMatrix(fTV_eval$tree_pred, fTV_eval$Y,  positive = "YES")
