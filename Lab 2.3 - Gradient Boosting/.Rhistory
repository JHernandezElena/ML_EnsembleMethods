install.packages("gbm")
## Load libraries ---------------------------------------------------------------
library(caret)
library(ggplot2)
library(ROCR) #for plotting ROC curves.
library(MLTools)
library(gbm)
rm(list=ls())
## Load file -------------------------------------------------------------------------------------
fdata = read.table("SimData.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
str(fdata)
fdata$Y = as.factor(fdata$Y)
str(fdata)
## Exploratory analysis --------------------------------------------------------------------------------
ggplot(fdata)+geom_point(aes(x=X1,y=X2,color=Y))
## Divide the data into training and validation sets ---------------------------------------------------
set.seed(150) #For replication
ratioTR = 0.8 #Percentage for training
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
#obtain training and validation sets
fTR = fdata[trainIndex,]
fTV = fdata[-trainIndex,]
## Initialize trainControl -----------------------------------------------------------------------
#  no cross-validation method is used
ctrl <- trainControl(method = "none",                      # method= "none" when no resampling is used
summaryFunction = twoClassSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = twoClassSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
#-------------------------------------------------
gbmGrid <- expand.grid(.n.trees = seq(100, 1000, by = 50), #tampoco se pueden poner mazo arboles
.interaction.depth = seq(1, 10, by = 1), #queremos que aprenda DESPACIO
.shrinkage = c(0.01, 0.1), # two typical values
.n.minobsinnode = c(1))
# Train the GBM
set.seed(150)
GBM.fit =  train(fTR[,c("X1","X2")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "gbm",
tuneGrid = gbmGrid,
trControl = ctrl_tune,    #use cv
metric = "ROC",
verbose = FALSE) # The gbm() function produces copious amounts of output, avoid printing a lot
GBM.fit #information about the resampling settings
plot(GBM.fit)
